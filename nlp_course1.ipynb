{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# counter based bag of words\n",
    "# gensim with frequency\n",
    "# gensim with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensimNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts cygdb.exe, cython.exe and cythonize.exe are installed in 'C:\\Users\\Lenovo\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached gensim-4.1.2-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.19.2)\n",
      "Collecting Cython==0.29.23\n",
      "  Using cached Cython-0.29.23-cp38-cp38-win_amd64.whl (1.7 MB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.5.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Installing collected packages: Cython, gensim\n",
      "Successfully installed Cython-0.29.23 gensim-4.1.2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#pip install  --user gensim\n",
    "#pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Text preprocessing is a method to clean the text data and make it ready to feed data to the model. Text data contains noise in various forms like emotions, punctuation, text in a different case.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [ w for w in word_tokenize(text.lower()) if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_stop_words = [t for t in tokens if t not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text', 4), ('data', 3)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(not_stop_words).most_common(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=['Text preprocessing is a method to clean the','text data and make it ready to feed data to the model','Text data contains noise in various forms like emotions, ','punctuation text in a different case.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [ word_tokenize(w.lower())  for w in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['text', 'preprocessing', 'is', 'a', 'method', 'to', 'clean', 'the'],\n",
       " ['text',\n",
       "  'data',\n",
       "  'and',\n",
       "  'make',\n",
       "  'it',\n",
       "  'ready',\n",
       "  'to',\n",
       "  'feed',\n",
       "  'data',\n",
       "  'to',\n",
       "  'the',\n",
       "  'model'],\n",
       " ['text',\n",
       "  'data',\n",
       "  'contains',\n",
       "  'noise',\n",
       "  'in',\n",
       "  'various',\n",
       "  'forms',\n",
       "  'like',\n",
       "  'emotions',\n",
       "  ','],\n",
       " ['punctuation', 'text', 'in', 'a', 'different', 'case', '.']]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_=Dictionary(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[dict_.doc2bow(doc) for doc in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(5, 1),\n",
       "  (6, 1),\n",
       "  (7, 2),\n",
       "  (8, 1),\n",
       "  (9, 2),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1)],\n",
       " [(5, 1),\n",
       "  (9, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1)],\n",
       " [(0, 1), (5, 1), (19, 1), (23, 1), (24, 1), (25, 1), (26, 1)]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'clean': 1,\n",
       " 'is': 2,\n",
       " 'method': 3,\n",
       " 'preprocessing': 4,\n",
       " 'text': 5,\n",
       " 'the': 6,\n",
       " 'to': 7,\n",
       " 'and': 8,\n",
       " 'data': 9,\n",
       " 'feed': 10,\n",
       " 'it': 11,\n",
       " 'make': 12,\n",
       " 'model': 13,\n",
       " 'ready': 14,\n",
       " ',': 15,\n",
       " 'contains': 16,\n",
       " 'emotions': 17,\n",
       " 'forms': 18,\n",
       " 'in': 19,\n",
       " 'like': 20,\n",
       " 'noise': 21,\n",
       " 'various': 22,\n",
       " '.': 23,\n",
       " 'case': 24,\n",
       " 'different': 25,\n",
       " 'punctuation': 26}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.22941573387056174),\n",
       " (1, 0.4588314677411235),\n",
       " (2, 0.4588314677411235),\n",
       " (3, 0.4588314677411235),\n",
       " (4, 0.4588314677411235),\n",
       " (6, 0.22941573387056174),\n",
       " (7, 0.22941573387056174)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacyNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading spacy-3.1.3-cp38-cp38-win_amd64.whl (12.0 MB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.5-cp38-cp38-win_amd64.whl (36 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.5-cp38-cp38-win_amd64.whl (112 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Using cached wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Using cached pydantic-1.8.2-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (50.3.1.post20201107)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (20.4)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.11.2)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.19.2)\n",
      "Collecting thinc<8.1.0,>=8.0.9\n",
      "  Downloading thinc-8.0.10-cp38-cp38-win_amd64.whl (1.0 MB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.0-py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (4.50.2)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.1-cp38-cp38-win_amd64.whl (451 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.5-cp38-cp38-win_amd64.whl (21 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.24.0)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Using cached blis-0.7.4-cp38-cp38-win_amd64.whl (6.5 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Installing collected packages: cymem, murmurhash, preshed, spacy-legacy, wasabi, pydantic, typer, catalogue, srsly, blis, thinc, pathy, spacy\n",
      "  Attempting uninstall: srsly\n",
      "    Found existing installation: srsly 1.0.5\n",
      "    Uninstalling srsly-1.0.5:\n",
      "      Successfully uninstalled srsly-1.0.5\n",
      "Successfully installed blis-0.7.4 catalogue-2.0.6 cymem-2.0.5 murmurhash-1.0.5 pathy-0.6.0 preshed-3.0.5 pydantic-1.8.2 spacy-3.1.3 spacy-legacy-3.0.8 srsly-2.4.1 thinc-8.0.10 typer-0.4.0 wasabi-0.8.2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/aboSamoor/polyglot.git@master\n",
      "  Cloning https://github.com/aboSamoor/polyglot.git (to revision master) to c:\\users\\lenovo\\appdata\\local\\temp\\pip-req-build-1lp4lq5n\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from polyglot==16.7.4) (0.35.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from polyglot==16.7.4) (1.19.2)\n",
      "Collecting futures>=2.1.6\n",
      "  Downloading futures-3.1.1-py3-none-any.whl (2.8 kB)\n",
      "Collecting pycld2>=0.3\n",
      "  Downloading pycld2-0.41.tar.gz (41.4 MB)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.7.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from polyglot==16.7.4) (1.15.0)\n",
      "Collecting PyICU>=1.8\n",
      "  Downloading PyICU-2.7.4.tar.gz (298 kB)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\ProgramData\\Anaconda3\\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Temp\\\\pip-install-ljx8f7cg\\\\pyicu\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Temp\\\\pip-install-ljx8f7cg\\\\pyicu\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-pip-egg-info-horuasz5'\n",
      "         cwd: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-install-ljx8f7cg\\pyicu\\\n",
      "    Complete output (53 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-install-ljx8f7cg\\pyicu\\setup.py\", line 63, in <module>\n",
      "        ICU_VERSION = os.environ['ICU_VERSION']\n",
      "      File \"C:\\ProgramData\\Anaconda3\\lib\\os.py\", line 675, in __getitem__\n",
      "        raise KeyError(key) from None\n",
      "    KeyError: 'ICU_VERSION'\n",
      "    \n",
      "    During handling of the above exception, another exception occurred:\n",
      "    \n",
      "    Traceback (most recent call last):\n",
      "      File \"C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-install-ljx8f7cg\\pyicu\\setup.py\", line 66, in <module>\n",
      "        ICU_VERSION = check_output(('icu-config', '--version')).strip()\n",
      "      File \"C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-install-ljx8f7cg\\pyicu\\setup.py\", line 19, in check_output\n",
      "        return subprocess_check_output(popenargs)\n",
      "      File \"C:\\ProgramData\\Anaconda3\\lib\\subprocess.py\", line 411, in check_output\n",
      "        return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
      "      File \"C:\\ProgramData\\Anaconda3\\lib\\subprocess.py\", line 489, in run\n",
      "        with Popen(*popenargs, **kwargs) as process:\n",
      "      File \"C:\\ProgramData\\Anaconda3\\lib\\subprocess.py\", line 854, in __init__\n",
      "        self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "      File \"C:\\ProgramData\\Anaconda3\\lib\\subprocess.py\", line 1307, in _execute_child\n",
      "        hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "    FileNotFoundError: [WinError 2] The system cannot find the file specified\n",
      "    \n",
      "    During handling of the above exception, another exception occurred:\n",
      "    \n",
      "    Traceback (most recent call last):\n",
      "      File \"C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-install-ljx8f7cg\\pyicu\\setup.py\", line 69, in <module>\n",
      "        ICU_VERSION = check_output(('pkg-config', '--modversion', 'icu-i18n')).strip()\n",
      "      File \"C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-install-ljx8f7cg\\pyicu\\setup.py\", line 19, in check_output\n",
      "        return subprocess_check_output(popenargs)\n",
      "      File \"C:\\ProgramData\\Anaconda3\\lib\\subprocess.py\", line 411, in check_output\n",
      "        return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
      "      File \"C:\\ProgramData\\Anaconda3\\lib\\subprocess.py\", line 489, in run\n",
      "        with Popen(*popenargs, **kwargs) as process:\n",
      "      File \"C:\\ProgramData\\Anaconda3\\lib\\subprocess.py\", line 854, in __init__\n",
      "        self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "      File \"C:\\ProgramData\\Anaconda3\\lib\\subprocess.py\", line 1307, in _execute_child\n",
      "        hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "    FileNotFoundError: [WinError 2] The system cannot find the file specified\n",
      "    \n",
      "    During handling of the above exception, another exception occurred:\n",
      "    \n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-install-ljx8f7cg\\pyicu\\setup.py\", line 71, in <module>\n",
      "        raise RuntimeError('''\n",
      "    RuntimeError:\n",
      "    Please install pkg-config on your system or set the ICU_VERSION environment\n",
      "    variable to the version of ICU you have installed.\n",
      "    \n",
      "    (running 'icu-config --version')\n",
      "    (running 'pkg-config --modversion icu-i18n')\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n"
     ]
    }
   ],
   "source": [
    "pip install -U git+https://github.com/aboSamoor/polyglot.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting auto-py-to-exe\n",
      "  Downloading auto_py_to_exe-2.10.0-py2.py3-none-any.whl (81 kB)\n",
      "Collecting Eel==0.12.4\n",
      "  Downloading Eel-0.12.4.tar.gz (15 kB)\n",
      "Collecting pyinstaller>=4.1\n",
      "  Downloading pyinstaller-4.5.1-py3-none-win_amd64.whl (1.9 MB)\n",
      "Collecting bottle\n",
      "  Downloading bottle-0.12.19-py3-none-any.whl (89 kB)\n",
      "Collecting bottle-websocket\n",
      "  Downloading bottle-websocket-0.2.9.tar.gz (2.0 kB)\n",
      "Requirement already satisfied: future in c:\\programdata\\anaconda3\\lib\\site-packages (from Eel==0.12.4->auto-py-to-exe) (0.18.2)\n",
      "Collecting whichcraft\n",
      "  Downloading whichcraft-0.6.1-py2.py3-none-any.whl (5.2 kB)\n",
      "Collecting pyinstaller-hooks-contrib>=2020.6\n",
      "  Downloading pyinstaller_hooks_contrib-2021.3-py2.py3-none-any.whl (200 kB)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from pyinstaller>=4.1->auto-py-to-exe) (50.3.1.post20201107)\n",
      "Requirement already satisfied: pywin32-ctypes>=0.2.0; sys_platform == \"win32\" in c:\\programdata\\anaconda3\\lib\\site-packages (from pyinstaller>=4.1->auto-py-to-exe) (0.2.0)\n",
      "Collecting altgraph\n",
      "  Downloading altgraph-0.17.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting pefile>=2017.8.1; sys_platform == \"win32\"\n",
      "  Downloading pefile-2021.9.3.tar.gz (72 kB)\n",
      "Collecting gevent-websocket\n",
      "  Downloading gevent_websocket-0.10.1-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: gevent in c:\\programdata\\anaconda3\\lib\\site-packages (from gevent-websocket->bottle-websocket->Eel==0.12.4->auto-py-to-exe) (20.9.0)\n",
      "Requirement already satisfied: greenlet>=0.4.17; platform_python_implementation == \"CPython\" in c:\\programdata\\anaconda3\\lib\\site-packages (from gevent->gevent-websocket->bottle-websocket->Eel==0.12.4->auto-py-to-exe) (0.4.17)\n",
      "Requirement already satisfied: zope.event in c:\\programdata\\anaconda3\\lib\\site-packages (from gevent->gevent-websocket->bottle-websocket->Eel==0.12.4->auto-py-to-exe) (4.5.0)\n",
      "Requirement already satisfied: cffi>=1.12.2; platform_python_implementation == \"CPython\" and sys_platform == \"win32\" in c:\\programdata\\anaconda3\\lib\\site-packages (from gevent->gevent-websocket->bottle-websocket->Eel==0.12.4->auto-py-to-exe) (1.14.3)\n",
      "Requirement already satisfied: zope.interface in c:\\programdata\\anaconda3\\lib\\site-packages (from gevent->gevent-websocket->bottle-websocket->Eel==0.12.4->auto-py-to-exe) (5.1.2)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.12.2; platform_python_implementation == \"CPython\" and sys_platform == \"win32\"->gevent->gevent-websocket->bottle-websocket->Eel==0.12.4->auto-py-to-exe) (2.20)\n",
      "Building wheels for collected packages: Eel, bottle-websocket, pefile\n",
      "  Building wheel for Eel (setup.py): started\n",
      "  Building wheel for Eel (setup.py): finished with status 'done'\n",
      "  Created wheel for Eel: filename=Eel-0.12.4-py3-none-any.whl size=16235 sha256=f4ef1e9087948aa31e88cd7662f1e7c7be308e956882dbd571aa9612a96dbcc8\n",
      "  Stored in directory: c:\\users\\lenovo\\appdata\\local\\pip\\cache\\wheels\\9d\\ca\\af\\6610598aa208b55dc7939fd98f9585d65b9376cfbe5d6eb0ab\n",
      "  Building wheel for bottle-websocket (setup.py): started\n",
      "  Building wheel for bottle-websocket (setup.py): finished with status 'done'\n",
      "  Created wheel for bottle-websocket: filename=bottle_websocket-0.2.9-py3-none-any.whl size=2353 sha256=06794677fab4cccb206b4dea1854e9fc78d7e7cdfef326986c2c6fcfbd58571b\n",
      "  Stored in directory: c:\\users\\lenovo\\appdata\\local\\pip\\cache\\wheels\\ab\\bd\\a8\\c939961e0cf3a5515ea922ef6b47e33c1c7237f0ac39443ed5\n",
      "  Building wheel for pefile (setup.py): started\n",
      "  Building wheel for pefile (setup.py): finished with status 'done'\n",
      "  Created wheel for pefile: filename=pefile-2021.9.3-py3-none-any.whl size=68835 sha256=766773439af9ab9f69915633e7fc79bda23574965a074735b5fc112147ee14ac\n",
      "  Stored in directory: c:\\users\\lenovo\\appdata\\local\\pip\\cache\\wheels\\ee\\a0\\e1\\ff2dafe5ae846f536acf0a4cc4a8b8d4ccb3044ab87f3ef174\n",
      "Successfully built Eel bottle-websocket pefile\n",
      "Installing collected packages: bottle, gevent-websocket, bottle-websocket, whichcraft, Eel, pyinstaller-hooks-contrib, altgraph, pefile, pyinstaller, auto-py-to-exe\n",
      "Successfully installed Eel-0.12.4 altgraph-0.17.2 auto-py-to-exe-2.10.0 bottle-0.12.19 bottle-websocket-0.2.9 gevent-websocket-0.10.1 pefile-2021.9.3 pyinstaller-4.5.1 pyinstaller-hooks-contrib-2021.3 whichcraft-0.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install auto-py-to-exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Print the head of df\n",
    "print(df.head())\n",
    "\n",
    "# Create a series to store the labels: y\n",
    "y = df.label\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33, random_state=53)\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Print the head of count_df\n",
    "print(count_df.head())\n",
    "\n",
    "# Print the head of tfidf_df\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Calculate the difference in columns: difference\n",
    "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "print(difference)\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0, 1, .1)\n",
    "\n",
    "# Create the list of alphas: alphas\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "   score = metrics.accuracy_score(y_test, pred)\n",
    "   return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "   print('Alpha: ', alpha)\n",
    "   print('Score: ', train_and_predict(alpha))\n",
    "   print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hyphen-', 'word-']\n"
     ]
    }
   ],
   "source": [
    "my_string = 'word hyphen-word word word-hyphen'\n",
    "print(re.findall(r\"\\w+-\", my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6800\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(len(stopwords.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
